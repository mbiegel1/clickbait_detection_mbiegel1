{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Clickbait Detection Notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting asttokens==2.0.5\n",
      "  Using cached asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
      "Collecting backcall==0.2.0\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: certifi==2021.10.8 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer==2.0.12 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (2.0.12)\n",
      "Requirement already satisfied: cycler==0.11.0 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.11.0)\n",
      "Collecting daal==2021.5.3\n",
      "  Using cached daal-2021.5.3-py2.py3-none-manylinux1_x86_64.whl (284.3 MB)\n",
      "Collecting daal4py==2021.5.3\n",
      "  Using cached daal4py-2021.5.3-py38-none-manylinux1_x86_64.whl (22.6 MB)\n",
      "Collecting debugpy==1.6.0\n",
      "  Using cached debugpy-1.6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.8 MB)\n",
      "Collecting decorator==5.1.1\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting entrypoints==0.4\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting executing==0.8.3\n",
      "  Using cached executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
      "Collecting fonttools==4.33.3\n",
      "  Using cached fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "Requirement already satisfied: idna==3.3 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (3.3)\n",
      "Collecting ipykernel==6.13.0\n",
      "  Using cached ipykernel-6.13.0-py3-none-any.whl (131 kB)\n",
      "Collecting ipython==8.3.0\n",
      "  Using cached ipython-8.3.0-py3-none-any.whl (750 kB)\n",
      "Collecting jedi==0.18.1\n",
      "  Using cached jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: joblib==1.1.0 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (1.1.0)\n",
      "Collecting jupyter-client==7.3.0\n",
      "  Using cached jupyter_client-7.3.0-py3-none-any.whl (130 kB)\n",
      "Collecting jupyter-core==4.10.0\n",
      "  Using cached jupyter_core-4.10.0-py3-none-any.whl (87 kB)\n",
      "Collecting kiwisolver==1.4.2\n",
      "  Using cached kiwisolver-1.4.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: matplotlib==3.5.1 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (3.5.1)\n",
      "Collecting matplotlib-inline==0.1.3\n",
      "  Using cached matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\n",
      "Collecting nest-asyncio==1.5.5\n",
      "  Using cached nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
      "Collecting networkx==2.8\n",
      "  Using cached networkx-2.8-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: numpy==1.22.3 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (1.22.3)\n",
      "Requirement already satisfied: packaging==21.3 in /home/mtbiegel/.local/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (21.3)\n",
      "Collecting pandas==1.4.2\n",
      "  Using cached pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Collecting parso==0.8.3\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Collecting pexpect==4.8.0\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting pickleshare==0.7.5\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting Pillow==9.1.0\n",
      "  Using cached Pillow-9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n",
      "    Pillow==9.1.0 from https://files.pythonhosted.org/packages/44/cf/ea13e8a564f7d69077832bd619f8c7dfb643a701a56dc26867aa4cc9f6d8/Pillow-9.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl#sha256=3f42364485bfdab19c1373b5cd62f7c5ab7cc052e19644862ec8f15bb8af289e (from -r requirements.txt (line 31)):\n",
      "        Expected sha256 3f42364485bfdab19c1373b5cd62f7c5ab7cc052e19644862ec8f15bb8af289e\n",
      "             Got        d0fc04d70468b369d6b942f91a2e6130509959f7833246548c3fc17f9033c4c2\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 1:</b>\n",
    "<br>This first part runs through training four classifiers on training data. Then, the classifiers are tested on either the training data or external data. The accuracy and f1 scores are found for each classifier.\n",
    "\n",
    "<br><u>The four classifiers are:</u>\n",
    "    <br>Multinomial Naive Bayes\n",
    "    <br>Stochastic Gradient Descent\n",
    "    <br>Perceptron\n",
    "    <br>Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Import cell\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Used to increase parallel computing on CPU\n",
    "# pip install scikit-learn-intelex\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data formated\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Reading data into pandas dataframe\n",
    "\n",
    "# Defining Constants\n",
    "clickbait_title_column = \"headline\"\n",
    "is_clickbait_column = \"clickbait\"\n",
    "\n",
    "# Pandas dataframe df\n",
    "df = pd.read_csv(\"input_data/clickbait_consensus.csv\")\n",
    "\n",
    "y = df[is_clickbait_column]\n",
    "X = df[clickbait_title_column]\n",
    "\n",
    "print(\"Data formated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data vectorized\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Vectorizing data in order for classifiers to use them\n",
    "# This is required because classifiers need to numerical data, \n",
    "#   so the titles (wihch are strings) need to be transformed numerically\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"Data vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------TEST DATA-----------------------------------------\n",
      "\n",
      "Training data chosen\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Asking user what data to test classifiers on\n",
    "\n",
    "print(\"-------------------------TEST DATA-----------------------------------------\\n\")\n",
    "\n",
    "which_data = input(\"Would you like to test on training data (enter 0) or pre-determined, external data (enter 1)? \")\n",
    "\n",
    "if (which_data == \"0\"):\n",
    "    print(\"Training data chosen\")\n",
    "    test_multi_titles = df[clickbait_title_column]\n",
    "    test_multi_titles_nparray = df[is_clickbait_column].to_numpy()\n",
    "\n",
    "    vectors_test = vectorizer.transform(test_multi_titles)\n",
    "\n",
    "else:\n",
    "    print(\"External data chosen\")\n",
    "    \n",
    "    test_df = pd.read_csv(\"input_data/clickbait_ratio_flattened.csv\")\n",
    "\n",
    "    test_multi_titles = test_df[clickbait_title_column]\n",
    "    test_multi_titles_nparray = test_df[is_clickbait_column].to_numpy()\n",
    "\n",
    "    vectors_test = vectorizer.transform(test_multi_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------MULTINOMIALNB PREDICTING-----------------------------------------\n",
      "\n",
      "Prediction: [1 1 1 ... 0 0 0]\n",
      "\n",
      "Total accuracy classification score: 0.99315625\n",
      "Total F1 classification score: 0.9931562017124583\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Multinomial Naive Bayes Classification\n",
    "\n",
    "print(\"-------------------------MULTINOMIALNB PREDICTING-----------------------------------------\\n\")\n",
    "\n",
    "multiNB_clf = MultinomialNB(alpha=0.00001)\n",
    "multiNB_clf.fit(vectors, y)\n",
    "\n",
    "\n",
    "multiNB_pred = multiNB_clf.predict(vectors_test)\n",
    "\n",
    "print(\"Prediction:\", multiNB_pred)\n",
    "print()\n",
    "\n",
    "acc_score_multiNB = metrics.accuracy_score(test_multi_titles_nparray, multiNB_pred)\n",
    "f1_score_multNB = metrics.f1_score(test_multi_titles_nparray, multiNB_pred, average='macro')\n",
    "\n",
    "print('Total accuracy classification score: {}'.format(acc_score_multiNB))\n",
    "print('Total F1 classification score: {}'.format(f1_score_multNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------STOCHSTIC GRADIENT DESCENT (SGD) PREDICTING---------------------------------------\n",
      "\n",
      "Prediction: [0 1 1 ... 0 0 0]\n",
      "\n",
      "Total accuracy classification score: 0.97165625\n",
      "Total F1 classification score: 0.9716427193401487\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Stochastic Gradient Descent Classification\n",
    "\n",
    "print(\"-------------------------STOCHSTIC GRADIENT DESCENT (SGD) PREDICTING---------------------------------------\\n\")\n",
    "\n",
    "SGD_clf = SGDClassifier(loss=\"huber\", penalty=\"l2\", max_iter=1000)\n",
    "SGD_clf.fit(vectors, y)\n",
    "\n",
    "SGD_pred = SGD_clf.predict(vectors_test)\n",
    "\n",
    "print(\"Prediction:\", SGD_pred)\n",
    "print()\n",
    "\n",
    "acc_score_SGD = metrics.accuracy_score(test_multi_titles_nparray, SGD_pred)\n",
    "f1_score_SGD = metrics.f1_score(test_multi_titles_nparray, SGD_pred, average='macro')\n",
    "\n",
    "print('Total accuracy classification score: {}'.format(acc_score_SGD))\n",
    "print('Total F1 classification score: {}'.format(f1_score_SGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------PERCEPTRON PREDICTING---------------------------------------\n",
      "\n",
      "Prediction: [1 1 1 ... 0 0 0]\n",
      "\n",
      "Total accuracy classification score: 0.9984375\n",
      "Total F1 classification score: 0.9984374975585899\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Perceptron Classification\n",
    "\n",
    "print(\"-------------------------PERCEPTRON PREDICTING---------------------------------------\\n\")\n",
    "\n",
    "perceptron_clf = Perceptron(tol=1e-3, random_state=0)\n",
    "perceptron_clf.fit(vectors, y)\n",
    "\n",
    "perceptron_pred = perceptron_clf.predict(vectors_test)\n",
    "\n",
    "print(\"Prediction:\", perceptron_pred)\n",
    "print()\n",
    "\n",
    "acc_score_perceptron = metrics.accuracy_score(test_multi_titles_nparray, perceptron_pred)\n",
    "f1_score_perceptron = metrics.f1_score(test_multi_titles_nparray, perceptron_pred, average='macro')\n",
    "\n",
    "print('Total accuracy classification score: {}'.format(acc_score_perceptron))\n",
    "print('Total F1 classification score: {}'.format(f1_score_perceptron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------SVM PREDICTING---------------------------------------\n",
      "\n",
      "Prediction: [1 1 0 ... 1 1 1]\n",
      "\n",
      "Total accuracy classification score: 0.80653125\n",
      "Total F1 classification score: 0.8065310911061012\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Support Vector Machine Classification\n",
    "\n",
    "print(\"-------------------------SVM PREDICTING---------------------------------------\\n\")\n",
    "\n",
    "SVM_clf = svm.SVC(gamma=3, kernel='sigmoid')\n",
    "SVM_clf.fit(vectors, y)\n",
    "\n",
    "SVM_pred = SVM_clf.predict(vectors_test)\n",
    "\n",
    "print(\"Prediction:\", SVM_pred)\n",
    "print()\n",
    "\n",
    "acc_score_SVM = metrics.accuracy_score(test_multi_titles_nparray, SVM_pred)\n",
    "f1_score_SVM = metrics.f1_score(test_multi_titles_nparray, SVM_pred, average='macro')\n",
    "\n",
    "print('Total accuracy classification score: {}'.format(acc_score_SVM))\n",
    "print('Total F1 classification score: {}'.format(f1_score_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 2</b>\n",
    "<br> Part 2 allows you, the user, to enter in your own titles that you find on the internet or create on your own and run it throught the four classifiers to see if it's clickbait.\n",
    "<br>\n",
    "<br> The notebook will walk through the functions for training the classifiers (same process as above, only it is formatted professionally now). Then, you will be prompted to enter text so that the classifiers can predict clickbait status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "# trainClassifiers() runs a compiled .csv file through all four classifiers of over 30,000 data points.\n",
    "\n",
    "def trainClassifiers():\n",
    "\n",
    "    print(\"\\nTraining classifiers...\", end=\"\")\n",
    "\n",
    "     # Defining Constants\n",
    "    clickbait_title_column = \"headline\"\n",
    "    is_clickbait_column = \"clickbait\"\n",
    "\n",
    "    # Pandas dataframe df\n",
    "    df = pd.read_csv(\"input_data/clickbait_compilation.csv\")\n",
    "\n",
    "\n",
    "    y = df[is_clickbait_column]\n",
    "    X = df[clickbait_title_column].str.lower() # All titles are in lowercase\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectors = vectorizer.fit_transform(X)\n",
    "\n",
    "\n",
    "    # MultinomialNB\n",
    "    multiNB_clf = MultinomialNB(alpha=0.00001)\n",
    "    multiNB_clf.fit(vectors, y)\n",
    "\n",
    "    # SGD\n",
    "    SGD_clf = SGDClassifier(loss=\"huber\", penalty=\"l2\", max_iter=1000)\n",
    "    SGD_clf.fit(vectors, y)\n",
    "\n",
    "    # Perceptron\n",
    "    perceptron_clf = Perceptron(tol=1e-3, random_state=0)\n",
    "    perceptron_clf.fit(vectors, y)\n",
    "\n",
    "    # SVM\n",
    "    SVM_clf = svm.SVC(gamma=3, kernel='sigmoid')\n",
    "    SVM_clf.fit(vectors, y)\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "    return vectorizer, multiNB_clf, SGD_clf, perceptron_clf, SVM_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "# predictWithClassifiers() takes the user's entry and predicts clickbait status on all four classifiers\n",
    "\n",
    "def predictWithClassifiers(user_clickbait_title, vectorizer, multiNB_pred, SGD_pred, perceptron_pred, SVM_pred):\n",
    "        \n",
    "        print(\"\\n-----------------------------------------------------------\")\n",
    "        print(\"STATISTICS:\")\n",
    "\n",
    "        # Vectorize the title to pass into the prediction of the classifier\n",
    "        vector_user_title_test = vectorizer.transform(user_clickbait_title)\n",
    "\n",
    "        # Multinomial Prediction\n",
    "        multiNB_pred = multiNB_clf.predict(vector_user_title_test)\n",
    "        print(\"Multinomial Naive Bayes Prediction:\\t\\t\", multiNB_pred)\n",
    "\n",
    "\n",
    "        # Stochastic Gradient Descent Prediction\n",
    "        SGD_pred = SGD_clf.predict(vector_user_title_test)\n",
    "        print(\"Stochastic Gradient Descent Prediction:\\t\\t\", SGD_pred)\n",
    "\n",
    "\n",
    "        # Perceptron Prediction\n",
    "        perceptron_pred = perceptron_clf.predict(vector_user_title_test)\n",
    "        print(\"Perceptron Prediction:\\t\\t\\t\\t\", perceptron_pred)\n",
    "        \n",
    "\n",
    "        # SVM prediction\n",
    "        SVM_pred = SVM_clf.predict(vector_user_title_test)\n",
    "        print(\"Support Vector Machine Prediction:\\t\\t\", SVM_pred)\n",
    "\n",
    "        return multiNB_pred, SGD_pred, perceptron_pred, SVM_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "# classifierClickbaitStatus()) finds the clickbait status of all four classifiers by averaging their predictions\n",
    "# Prediction values are [0] for a non-clickbait title and [1] for a clickbait title\n",
    "\n",
    "def classifierClickbaitRatio(multiNB_pred, SGD_pred, perceptron_pred, SVM_pred):\n",
    "    classifier_clickbait_ratio = (multiNB_pred+SGD_pred+perceptron_pred+SVM_pred)/ num_of_classifiers\n",
    "\n",
    "    print(\"\\nTotal Classification Clickbait Status:\\t\\t\\t\", classifier_clickbait_ratio)\n",
    "    print(\"\\n-----------------------------------------------------------\")\n",
    "\n",
    "    return classifier_clickbait_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "# determineClickbait() determines if the title is clickbait based on the four classifier predictions\n",
    "\n",
    "def determineClickbait(classifier_clickbait_ratio, isClickbait_threshold, possiblyClickbait_threshold):\n",
    "    # If the classifier accuracy is above the threshold, title is clickbait\n",
    "    # Else, it's below the threshold, so title is not clickbait\n",
    "    \n",
    "    if (classifier_clickbait_ratio >= isClickbait_threshold):\n",
    "        print(\"\\nCLICKBAIT\")\n",
    "    elif (classifier_clickbait_ratio == possiblyClickbait_threshold):\n",
    "        print(\"\\nPOSSIBLY CLICKBAIT\")\n",
    "    else:\n",
    "        print(\"\\nNOT CLICKBAIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training classifiers...done\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "\n",
    "# This cell trains the classifiers on the training data\n",
    "\n",
    "# train Classifiers on over 30,000 data points\n",
    "vectorizer, multiNB_clf, SGD_clf, perceptron_clf, SVM_clf = trainClassifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------\n",
      "STATISTICS:\n",
      "Multinomial Naive Bayes Prediction:\t\t [0]\n",
      "Stochastic Gradient Descent Prediction:\t\t [1]\n",
      "Perceptron Prediction:\t\t\t\t [1]\n",
      "Support Vector Machine Prediction:\t\t [0]\n",
      "\n",
      "Total Classification Clickbait Status:\t\t\t [0.5]\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "POSSIBLY CLICKBAIT\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "\n",
    "# This cell asks for user input and runs prediction on the input\n",
    "# Displays results of prediction as well as clickbait determination\n",
    "\n",
    "# Constants\n",
    "num_of_classifiers = 4\n",
    "isClickbait_threshold = 0.75\n",
    "possiblyClickbait_threshold = 0.5\n",
    "\n",
    "# Get user clickbait title\n",
    "user_clickbait_title = input(\"\\n\\nEnter a title to see if it's clickbait (-1 to quit):\\n\").lower()\n",
    "\n",
    "if (user_clickbait_title != \"-1\"):\n",
    "\n",
    "    # Turn string title into list iterable for vectorization\n",
    "    user_clickbait_title = [user_clickbait_title]\n",
    "\n",
    "    # Predicting the clickbait classification on all classifiers\n",
    "    multiNB_pred, SGD_pred, perceptron_pred, SVM_pred = predictWithClassifiers(user_clickbait_title, vectorizer, multiNB_clf, SGD_clf, perceptron_clf, SVM_clf)\n",
    "\n",
    "    # Total mean classifier accuracy to determine if the title is clickbait\n",
    "    classifier_clickbait_ratio = classifierClickbaitRatio(multiNB_pred, SGD_pred, perceptron_pred, SVM_pred)\n",
    "\n",
    "    # Determing that clickbait status of the user's title\n",
    "    determineClickbait(classifier_clickbait_ratio, isClickbait_threshold, possiblyClickbait_threshold)\n",
    "\n",
    "else:\n",
    "    print(\"Bye now!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "478_kernel",
   "language": "python",
   "name": "478_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
